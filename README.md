
# üìò Knowledge-Base Search Engine (RAG System)
 **A smart document search engine using Retrieval-Augmented Generation (RAG).**
 
## üé• Demo Video
Watch the project demo here:  
üëâ **[Google Drive Video Demo](https://drive.google.com/drive/folders/1cqlM7S8ptZMbOkUGsVEJghHE-YQ08s7v?usp=sharing)**

---

## üìå Project Overview

This project implements a **Knowledge-Base Search Engine** using  
**Retrieval-Augmented Generation (RAG)**.

The system allows users to upload documents (**TXT / PDF**), ask natural language
questions, and receive **accurate, document-grounded answers** generated by a
Large Language Model (LLM).

The core idea is to **retrieve relevant document content using embeddings** and
then **synthesize answers using an LLM**, instead of relying solely on the
model‚Äôs internal knowledge.

---

## üéØ Objective

- Enable search and question answering over user-provided documents
- Reduce hallucinations by grounding answers in document content
- Demonstrate a complete RAG pipeline with backend APIs
- Handle real-world document ingestion challenges (PDFs)

---

## üß† What is RAG?

**Retrieval-Augmented Generation (RAG)** combines:

- **Information Retrieval** ‚Äì finding relevant document chunks
- **Language Generation** ‚Äì generating answers using an LLM with retrieved context

### Benefits
- Improved accuracy
- Higher reliability
- Better explainability
- More up-to-date knowledge

---

## üèóÔ∏è System Architecture

```

User
‚îÇ
‚îú‚îÄ‚îÄ Upload Document (PDF / TXT)
‚îÇ
‚ñº
Document Ingestion
‚îú‚îÄ‚îÄ Text Extraction
‚îú‚îÄ‚îÄ Cleaning & Chunking
‚îú‚îÄ‚îÄ Embedding Generation (Gemini)
‚îÇ
‚ñº
In-Memory Vector Store
‚îÇ
User Query
‚îÇ
‚îú‚îÄ‚îÄ Query Embedding (Gemini)
‚îú‚îÄ‚îÄ Cosine Similarity Search
‚îÇ
‚ñº
Top-K Relevant Chunks
‚îÇ
‚ñº
LLM Answer Generation (Groq ‚Äì LLaMA)
‚îÇ
‚ñº
Final Answer

```

---

## üõ†Ô∏è Tech Stack

### Backend
- Node.js
- Express.js

### AI / ML
- Google Gemini API ‚Äì Embeddings
- Groq API (LLaMA-3.1) ‚Äì Answer generation

### Document Processing
- pdf-parse ‚Äì PDF text extraction
- Custom text cleaning & validation

### Utilities
- Multer ‚Äì File uploads
- Cosine similarity ‚Äì Vector search

---

## üìÇ Project Structure

```

knowledge-base-rag/
‚îÇ
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ server.js        # Express server & routes
‚îÇ   ‚îú‚îÄ‚îÄ ingest.js        # Document ingestion logic
‚îÇ   ‚îú‚îÄ‚îÄ embed.js         # Gemini embedding generation
‚îÇ   ‚îú‚îÄ‚îÄ query.js         # Retrieval + Groq generation
‚îÇ   ‚îú‚îÄ‚îÄ search.js        # Cosine similarity
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ documents/       # Uploaded files (temporary)
‚îÇ
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ package.json
‚îî‚îÄ‚îÄ README.md

```

---

## üîÑ API Endpoints

### üì§ Upload Document
```

POST /upload

````

- Content-Type: `multipart/form-data`
- Field name: `file`
- Supports: `.txt`, text-based `.pdf`

**Response**
```json
{
  "message": "Document ingested successfully"
}
````

---

### ‚ùì Ask a Question

```
POST /query
```

**Request**

```json
{
  "question": "What is Retrieval Augmented Generation?"
}
```

**Response**

```json
{
  "answer": "Retrieval Augmented Generation is a technique where relevant documents..."
}
```

---

## üîç How the System Works (Step by Step)

### 1Ô∏è‚É£ Document Ingestion

* User uploads a document
* Text is extracted
* Text is cleaned and validated
* Content is split into chunks
* Each chunk is converted into an embedding (Gemini)

### 2Ô∏è‚É£ Vector Storage

* Embeddings stored in an **in-memory vector store**
* Each entry contains `{ text, embedding }`

### 3Ô∏è‚É£ Query Processing

* User query ‚Üí embedding (Gemini)
* Cosine similarity used to find relevant chunks

### 4Ô∏è‚É£ Answer Generation

* Top-K chunks passed as context
* Groq LLM generates an answer strictly from context

---

## ü§ñ Why Gemini + Groq?

### üîπ Gemini (Embeddings)

* Free tier
* High-quality semantic embeddings
* Ideal for retrieval tasks

### üîπ Groq (Generation)

* Free and fast
* Stable OpenAI-compatible API
* Avoids Gemini SDK instability for text generation

> The RAG architecture is provider-agnostic, so embeddings and generation can use
> different models without affecting system design.

---

## üöß Challenges Faced & Solutions

### ‚ùå 1. PDF Text Extraction Issues

**Problem**

* Some PDFs returned binary garbage (`/Font`, `endobj`, symbols)

**Cause**

* Font-encoded or encrypted PDFs
* PDF is not a true text document

**Solution**

* Strong text cleaning
* Validation to reject unsupported PDFs
* Recommendation to use text-based PDFs or OCR

---

### ‚ùå 2. Gemini Model Errors (404 / Not Supported)

**Problem**

* Gemini `generateContent` models returned errors

**Cause**

* SDK version and API instability

**Solution**

* Switched to Groq for text generation
* Retained Gemini for embeddings
* Improved reliability

---

### ‚ùå 3. Slow Ingestion Time

**Problem**

* Large PDFs caused slow embedding generation

**Cause**

* Sequential embedding calls

**Solution**

* Limited chunk count for demo
* Explained ingestion as an offline step

---

### ‚ùå 4. Data Loss on Server Restart

**Problem**

* Vector database resets on restart

**Cause**

* In-memory storage

**Solution**

* Documented limitation
* Explained production alternatives (FAISS, Chroma, MongoDB Atlas)

---

## üìÑ PDF Support (Important Note)

This system supports **text-based PDFs only**.

### ‚úî Works well with

* LibreOffice PDFs
* Research papers
* Word-exported PDFs

### ‚ùå Not supported

* Scanned PDFs
* Image-only PDFs
* Font-encoded PDFs (Google Docs / Canva)

> OCR can be integrated in future versions to support scanned PDFs.

---

## ‚ñ∂Ô∏è How to Run Locally

### 1Ô∏è‚É£ Clone Repository

```bash
git clone https://github.com/your-username/knowledge-base-rag.git
cd knowledge-base-rag
```

### 2Ô∏è‚É£ Install Dependencies

```bash
npm install
```

### 3Ô∏è‚É£ Setup Environment Variables

Create a `.env` file:

```env
GEMINI_API_KEY=your_gemini_key
GROQ_API_KEY=your_groq_key
```

### 4Ô∏è‚É£ Start Server

```bash
node backend/server.js
```

Server runs at:

```
http://localhost:3000
```

---

## üß™ Demo Flow

1. Upload a TXT or text-based PDF
2. Wait for ingestion
3. Ask a question
4. Receive a grounded AI answer

```

---

