ğŸ“˜ Knowledge-Base Search Engine (RAG System)
   ## ğŸ¥ Demo Video

  Watch the project demo here:  
  ğŸ‘‰ [Google Drive Video Demo](https://drive.google.com/drive/folders/1cqlM7S8ptZMbOkUGsVEJghHE-YQ08s7v?usp=sharing)
ğŸ“Œ Project Overview

This project implements a Knowledge-Base Search Engine using Retrieval-Augmented Generation (RAG).
The system allows users to upload documents (TXT / PDF), ask natural language questions, and receive accurate, document-grounded answers generated by a Large Language Model (LLM).

The core idea is to retrieve relevant document content using embeddings and then synthesize answers using an LLM, instead of relying solely on the modelâ€™s internal knowledge.

ğŸ¯ Objective

Enable search and question answering over user-provided documents

Reduce hallucinations by grounding answers in document content

Demonstrate a complete RAG pipeline with backend APIs

Handle real-world document ingestion challenges (PDFs)

ğŸ§  What is RAG?

Retrieval-Augmented Generation (RAG) combines:

Information Retrieval â€“ finding relevant document chunks

Language Generation â€“ generating answers using an LLM with retrieved context

This improves:

Accuracy

Reliability

Explainability

Up-to-date knowledge

ğŸ—ï¸ System Architecture
User
  â”‚
  â”œâ”€â”€ Upload Document (PDF / TXT)
  â”‚
  â–¼
Document Ingestion
  â”œâ”€â”€ Text Extraction
  â”œâ”€â”€ Cleaning & Chunking
  â”œâ”€â”€ Embedding Generation (Gemini)
  â”‚
  â–¼
In-Memory Vector Store
  â”‚
User Query
  â”‚
  â”œâ”€â”€ Query Embedding (Gemini)
  â”œâ”€â”€ Cosine Similarity Search
  â”‚
  â–¼
Top-K Relevant Chunks
  â”‚
  â–¼
LLM Answer Generation (Groq â€“ LLaMA)
  â”‚
  â–¼
Final Answer

ğŸ› ï¸ Tech Stack
Backend

Node.js

Express.js

AI / ML

Google Gemini API â€“ embeddings

Groq API (LLaMA-3.1) â€“ answer generation

Document Processing

pdf-parse â€“ PDF text extraction

Custom text cleaning & validation

Utilities

Multer â€“ file uploads

Cosine similarity â€“ vector search

ğŸ“‚ Project Structure
Knowledge-base-rag/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.js        # Express server & routes
â”‚   â”œâ”€â”€ ingest.js        # Document ingestion logic
â”‚   â”œâ”€â”€ embed.js         # Gemini embedding generation
â”‚   â”œâ”€â”€ query.js         # Retrieval + Groq generation
â”‚   â”œâ”€â”€ search.js        # Cosine similarity
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ documents/       # Uploaded files (temporary)
â”‚
â”œâ”€â”€ .env.example         # Environment variables template
â”œâ”€â”€ package.json
â””â”€â”€ README.md

ğŸ”„ API Endpoints
ğŸ“¤ Upload Document
POST /upload


Content-Type: multipart/form-data

Field name: file

Supports: .txt, text-based .pdf

Response

{
  "message": "Document ingested successfully"
}

â“ Ask a Question
POST /query


Request

{
  "question": "What is Retrieval Augmented Generation?"
}


Response

{
  "answer": "Retrieval Augmented Generation is a technique where relevant documents..."
}

ğŸ” How the System Works (Step by Step)
1ï¸âƒ£ Document Ingestion

User uploads a document

Text is extracted

Text is cleaned and validated

Content is split into chunks

Each chunk is converted into an embedding (Gemini)

2ï¸âƒ£ Vector Storage

Embeddings are stored in an in-memory vector store

Each entry = { text, embedding }

3ï¸âƒ£ Query Processing

User query â†’ embedding (Gemini)

Cosine similarity used to find relevant chunks

4ï¸âƒ£ Answer Generation

Top-K chunks passed as context

Groq LLM generates an answer strictly from context

ğŸ¤– Why Gemini + Groq?
ğŸ”¹ Gemini (Embeddings)

Free tier

High-quality semantic embeddings

Ideal for retrieval tasks

ğŸ”¹ Groq (Generation)

Free and fast

Stable OpenAI-compatible API

Avoids Gemini SDK instability for text generation

The RAG architecture is provider-agnostic, so embeddings and generation can use different models without affecting system design.

ğŸš§ Challenges Faced & Solutions
âŒ 1. PDF Text Extraction Issues

Problem

Some PDFs returned binary garbage (/Font, endobj, symbols)

Cause

Font-encoded or encrypted PDFs

PDF is not a true text document

Solution

Strong text cleaning

Validation to reject bad PDFs

Recommendation to use text-based PDFs or OCR

âŒ 2. Gemini Model Errors (404 / Not Supported)

Problem

Gemini generateContent models returned 404 errors

Cause

SDK version & API instability

Solution

Switched to Groq for text generation

Retained Gemini for embeddings

Improved reliability

âŒ 3. Slow Ingestion Time

Problem

Large PDFs caused slow embedding generation

Cause

Sequential embedding calls

Solution

Limited chunk count for demo

Explained as offline ingestion step

âŒ 4. Data Loss on Server Restart

Problem

Vector DB resets on restart

Cause

In-memory storage

Solution

Documented limitation

Explained production alternatives (FAISS, Chroma, MongoDB Atlas)

ğŸ“„ PDF Support (Important Note)

This system supports text-based PDFs only.

âœ” Works well with:

LibreOffice PDFs

Research papers

Word-exported PDFs

âŒ Not supported:

Scanned PDFs

Image-only PDFs

Font-encoded PDFs (Google Docs / Canva)

OCR can be integrated in future versions to support scanned PDFs.

â–¶ï¸ How to Run Locally
1ï¸âƒ£ Clone Repository
git clone https://github.com/your-username/knowledge-base-rag.git
cd knowledge-base-rag

2ï¸âƒ£ Install Dependencies
npm install

3ï¸âƒ£ Setup Environment Variables

Create .env file:

GEMINI_API_KEY=your_gemini_key
GROQ_API_KEY=your_groq_key

4ï¸âƒ£ Start Server
node backend/server.js


Server runs on:

http://localhost:3000

ğŸ§ª Demo Flow

Upload a TXT or text-based PDF

Wait for ingestion

Ask a question

Receive grounded answer