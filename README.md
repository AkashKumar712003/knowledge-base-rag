<h1 align="center">ğŸ“˜ Knowledge-Base Search Engine (RAG System)</h1>

<p align="center">
  AI-powered PDF & TXT reader using <b>Retrieval-Augmented Generation (RAG)</b>
</p>

<p align="center">
  <b>Node.js</b> â€¢ <b>Gemini Embeddings</b> â€¢ <b>Groq LLM</b>
</p>

<hr/>

## ğŸ¥ Demo Video

ğŸ‘‰ **Watch the project demo here:**  
**Google Drive Video Demo** (add your link here)

> Upload a document â†’ Ask a question â†’ Get a document-grounded AI answer

---

## ğŸ“Œ Project Overview

This project implements a **Knowledge-Base Search Engine** using  
**Retrieval-Augmented Generation (RAG)**.

The system allows users to upload **TXT or PDF documents**, ask natural language
questions, and receive **accurate answers grounded strictly in the uploaded
documents**.

Instead of relying only on an LLMâ€™s internal knowledge, the system first
**retrieves relevant document content using embeddings**, then synthesizes the
final answer using an LLM.

---

## ğŸ¯ Objectives

- Enable question answering over user-provided documents  
- Reduce hallucinations by grounding answers in document context  
- Demonstrate a complete **RAG pipeline with backend APIs**  
- Handle real-world document ingestion challenges (PDFs)

---

## ğŸ§  What is Retrieval-Augmented Generation (RAG)?

Retrieval-Augmented Generation (RAG) combines:

- **Information Retrieval** â€“ finding relevant document chunks  
- **Language Generation** â€“ generating answers using an LLM with retrieved context  

### Benefits:
- Improved accuracy  
- Higher reliability  
- Better explainability  
- Up-to-date responses  

---

## ğŸ—ï¸ System Architecture

> Rendered inside a code block to keep GitHub formatting clean

User
â”‚
â”œâ”€â”€ Upload Document (PDF / TXT)
â”‚
â–¼
Document Ingestion
â”œâ”€â”€ Text Extraction
â”œâ”€â”€ Cleaning & Chunking
â”œâ”€â”€ Embedding Generation (Gemini)
â”‚
â–¼
In-Memory Vector Store
â”‚
User Query
â”‚
â”œâ”€â”€ Query Embedding (Gemini)
â”œâ”€â”€ Cosine Similarity Search
â”‚
â–¼
Top-K Relevant Chunks
â”‚
â–¼
LLM Answer Generation (Groq â€“ LLaMA)
â”‚
â–¼
Final Answer


---

## ğŸ› ï¸ Tech Stack

### Backend
- **Node.js**
- **Express.js**

### AI / ML
- **Google Gemini API** â€“ Embeddings
- **Groq API (LLaMA-3.1)** â€“ Answer generation

### Document Processing
- **pdf-parse** â€“ PDF text extraction
- Custom text cleaning and validation

### Utilities
- **Multer** â€“ File uploads
- **Cosine similarity** â€“ Vector search

---

## ğŸ“‚ Project Structure

knowledge-base-rag/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ server.js # Express server & routes
â”‚ â”œâ”€â”€ ingest.js # Document ingestion logic
â”‚ â”œâ”€â”€ embed.js # Gemini embedding generation
â”‚ â”œâ”€â”€ query.js # Retrieval + Groq generation
â”‚ â”œâ”€â”€ search.js # Cosine similarity
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ documents/ # Uploaded files (temporary)
â”‚
â”œâ”€â”€ .env.example
â”œâ”€â”€ package.json
â””â”€â”€ README.md


---

## ğŸ”„ API Endpoints

### ğŸ“¤ Upload Document

- Content-Type: `multipart/form-data`
- Field name: `file`
- Supports: `.txt`, text-based `.pdf`

**Response**
```json
{
  "message": "Document ingested successfully"
}
POST /query
{
  "question": "What is Retrieval Augmented Generation?"
}
{
  "answer": "Retrieval Augmented Generation is a technique where relevant documents..."
}
ğŸ” How the System Works
1ï¸âƒ£ Document Ingestion

User uploads a document

Text is extracted

Text is cleaned and validated

Content is split into chunks

Each chunk is converted into an embedding (Gemini)

2ï¸âƒ£ Vector Storage

Embeddings stored in an in-memory vector store

Each entry contains { text, embedding }

3ï¸âƒ£ Query Processing

User query â†’ embedding (Gemini)

Cosine similarity used to retrieve relevant chunks

4ï¸âƒ£ Answer Generation

Top-K chunks passed as context

Groq LLM generates an answer strictly from context

ğŸ¤– Why Gemini + Groq?
ğŸ”¹ Gemini (Embeddings)

Free tier

High-quality semantic embeddings

Ideal for retrieval tasks

ğŸ”¹ Groq (Generation)

Free and fast

Stable OpenAI-compatible API

Avoids Gemini SDK instability for text generation

The RAG architecture is provider-agnostic, allowing different providers for
retrieval and generation without changing the system design.

ğŸš§ Challenges Faced & Solutions
âŒ 1. PDF Text Extraction Issues

Problem:
Some PDFs returned binary garbage (/Font, endobj, symbols)

Cause:
Font-encoded or encrypted PDFs

Solution:

Strong text cleaning

Validation to reject unsupported PDFs

Recommendation to use text-based PDFs or OCR

âŒ 2. Gemini Model Errors (404 / Unsupported)

Problem:
Gemini generation models returned errors

Solution:

Retained Gemini for embeddings

Switched to Groq for text generation

Improved reliability without changing architecture

âŒ 3. Slow Ingestion Time

Problem:
Large PDFs caused slow embedding generation

Solution:

Limited chunk count for demo

Explained ingestion as an offline step

âŒ 4. Data Loss on Server Restart

Problem:
Vector DB resets on restart

Cause:
In-memory storage

Solution:

Documented limitation

Suggested production alternatives:
FAISS, Chroma, MongoDB Atlas Vector Search

ğŸ“„ PDF Support (Important Note)

âœ” Supported:

Text-based PDFs (LibreOffice, research papers)

TXT files

âŒ Not supported:

Scanned PDFs

Image-only PDFs

Font-encoded PDFs (Google Docs / Canva)

OCR can be integrated in future versions.

â–¶ï¸ How to Run Locally
1ï¸âƒ£ Clone Repository

git clone https://github.com/your-username/knowledge-base-rag.git
cd knowledge-base-rag

npm install

3ï¸âƒ£ Setup Environment Variables

Create .env file:

GEMINI_API_KEY=your_gemini_key
GROQ_API_KEY=your_groq_key

node backend/server.js

http://localhost:3000

ğŸ§ª Demo Flow

Upload a TXT or text-based PDF

Wait for ingestion

Ask a question

Receive a grounded AI answer

ğŸ Conclusion

This project demonstrates a complete, real-world RAG pipeline while handling
practical challenges such as PDF extraction, model instability, and vector
storage limitations.

The focus is on robust architecture, clarity, and explainability rather than
over-engineering.