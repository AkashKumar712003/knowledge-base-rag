Here is the formatted content in Markdown, ready to be pasted into your `README.md` file. I have enhanced the structure with syntax highlighting, tables, and a **Mermaid.js** diagram for the architecture, which GitHub renders automatically.

```markdown
# üìò Knowledge-Base Search Engine (RAG System)

> **A smart document search engine using Retrieval-Augmented Generation (RAG).**

## üé• Demo Video
Watch the project demo here:  
üëâ **[Google Drive Video Demo](https://drive.google.com/drive/folders/1cqlM7S8ptZMbOkUGsVEJghHE-YQ08s7v?usp=sharing)**

---

## üìå Project Overview

This project implements a **Knowledge-Base Search Engine** using **Retrieval-Augmented Generation (RAG)**. The system allows users to upload documents (TXT / PDF), ask natural language questions, and receive accurate, document-grounded answers generated by a Large Language Model (LLM).

The core idea is to retrieve relevant document content using **embeddings** and then synthesize answers using an **LLM**, instead of relying solely on the model‚Äôs internal knowledge.

### üéØ Objective
* Enable search and question answering over user-provided documents.
* Reduce hallucinations by grounding answers in document content.
* Demonstrate a complete RAG pipeline with backend APIs.
* Handle real-world document ingestion challenges (e.g., PDF parsing).

### üß† What is RAG?
**Retrieval-Augmented Generation (RAG)** combines:
1.  **Information Retrieval:** Finding relevant document chunks.
2.  **Language Generation:** Generating answers using an LLM with the retrieved context.

**Benefits:**
* ‚úÖ Accuracy
* ‚úÖ Reliability
* ‚úÖ Explainability
* ‚úÖ Up-to-date knowledge

---

## üèóÔ∏è System Architecture

```mermaid
graph TD
    User([User]) -->|Upload PDF/TXT| Ingest[Document Ingestion]
    
    subgraph "Ingestion Pipeline"
        Ingest --> Extract[Text Extraction]
        Extract --> Clean[Cleaning & Chunking]
        Clean --> EmbedGen[Embedding Generation (Gemini)]
        EmbedGen --> VectorDB[(In-Memory Vector Store)]
    end

    User -->|Ask Question| Query[User Query]
    
    subgraph "RAG Pipeline"
        Query --> QEmbed[Query Embedding (Gemini)]
        QEmbed --> Search[Cosine Similarity Search]
        Search -->|Retrieve Top-K Chunks| Context[Relevant Context]
        Context --> LLM[LLM Answer Generation (Groq - LLaMA)]
        LLM --> Answer[Final Answer]
    end
    
    Answer --> User

```

---

## üõ†Ô∏è Tech Stack

| Component | Technology | Description |
| --- | --- | --- |
| **Backend** | **Node.js, Express.js** | Server & API handling |
| **Embeddings** | **Google Gemini API** | Semantic vector generation |
| **LLM** | **Groq API (LLaMA-3.1)** | Fast, accurate answer generation |
| **PDF Processing** | **pdf-parse** | PDF text extraction |
| **Utilities** | **Multer** | File uploads |
| **Search** | **Cosine Similarity** | Vector search logic |

---

## üìÇ Project Structure

```bash
knowledge-base-rag/
‚îÇ
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ server.js       # Express server & routes
‚îÇ   ‚îú‚îÄ‚îÄ ingest.js       # Document ingestion logic
‚îÇ   ‚îú‚îÄ‚îÄ embed.js        # Gemini embedding generation
‚îÇ   ‚îú‚îÄ‚îÄ query.js        # Retrieval + Groq generation
‚îÇ   ‚îú‚îÄ‚îÄ search.js       # Cosine similarity logic
‚îÇ   ‚îú‚îÄ‚îÄ data/           # Data storage (mock/temp)
‚îÇ   ‚îî‚îÄ‚îÄ documents/      # Uploaded files (temporary)
‚îÇ
‚îú‚îÄ‚îÄ .env.example        # Environment variables template
‚îú‚îÄ‚îÄ package.json        # Dependencies
‚îî‚îÄ‚îÄ README.md           # Project documentation

```

---

## üîÑ API Endpoints

### üì§ Upload Document

**POST** `/upload`

* **Content-Type:** `multipart/form-data`
* **Field name:** `file`
* **Supports:** `.txt`, text-based `.pdf`

**Response:**

```json
{
  "message": "Document ingested successfully"
}

```

### ‚ùì Ask a Question

**POST** `/query`

**Request:**

```json
{
  "question": "What is Retrieval Augmented Generation?"
}

```

**Response:**

```json
{
  "answer": "Retrieval Augmented Generation is a technique where relevant documents..."
}

```

---

## üîç How the System Works (Step by Step)

### 1Ô∏è‚É£ Document Ingestion

* User uploads a document.
* Text is extracted, cleaned, and validated.
* Content is split into manageable chunks.
* Each chunk is converted into an embedding using **Gemini**.

### 2Ô∏è‚É£ Vector Storage

* Embeddings are stored in an **in-memory vector store**.
* Structure: `{ text, embedding }`.

### 3Ô∏è‚É£ Query Processing

* User query is converted into an embedding (**Gemini**).
* **Cosine similarity** is used to find the most relevant chunks from the store.

### 4Ô∏è‚É£ Answer Generation

* The Top-K relevant chunks are passed as context.
* **Groq LLM** generates an answer strictly based on the provided context.

---

## ü§ñ Why Gemini + Groq?

### üîπ Gemini (Embeddings)

* **Free tier available.**
* High-quality semantic embeddings.
* Ideal for retrieval tasks.

### üîπ Groq (Generation)

* **Free and extremely fast.**
* Stable OpenAI-compatible API.
* Avoids Gemini SDK instability for text generation tasks.

> The RAG architecture is provider-agnostic, meaning you can swap models without breaking the system design.

---

## üöß Challenges Faced & Solutions

| Problem | Cause | Solution |
| --- | --- | --- |
| **PDF Text Extraction Issues** | Font-encoded or encrypted PDFs returning binary garbage. | Implemented strong text cleaning and validation to reject bad PDFs. |
| **Gemini Model Errors (404)** | SDK version & API instability for generation. | Switched to **Groq** for text generation while keeping Gemini for embeddings. |
| **Slow Ingestion Time** | Large PDFs caused sequential embedding calls. | Limited chunk count for the demo and explained it as an offline step. |
| **Data Loss on Restart** | In-memory storage resets on server stop. | Documented the limitation; suggested **FAISS, ChromaDB, or MongoDB Atlas** for production. |

---

## üìÑ PDF Support (Important Note)

> ‚ö†Ô∏è **This system supports text-based PDFs only.**

| **Supported** ‚úîÔ∏è | **Not Supported** ‚ùå |
| --- | --- |
| LibreOffice PDFs | Scanned PDFs |
| Research Papers | Image-only PDFs |
| Word-exported PDFs | Font-encoded PDFs (e.g., some Canva exports) |

*Note: OCR integration is required to support scanned documents.*

---

## ‚ñ∂Ô∏è How to Run Locally

### 1Ô∏è‚É£ Clone Repository

```bash
git clone [https://github.com/your-username/knowledge-base-rag.git](https://github.com/your-username/knowledge-base-rag.git)
cd knowledge-base-rag

```

### 2Ô∏è‚É£ Install Dependencies

```bash
npm install

```

### 3Ô∏è‚É£ Setup Environment Variables

Create a `.env` file in the root directory:

```env
GEMINI_API_KEY=your_gemini_key
GROQ_API_KEY=your_groq_key

```

### 4Ô∏è‚É£ Start Server

```bash
node backend/server.js

```

The server will run on: `http://localhost:3000`

---

## üß™ Demo Flow

1. Upload a **TXT** or **text-based PDF**.
2. Wait for the ingestion success message.
3. Ask a question about the document.
4. Receive a grounded answer! üöÄ

```

```